---
description: Chat API System Design Report
globs: 
alwaysApply: false
---
# Chat API System Design Report

## 1. Overview

This system facilitates multi-turn conversations with a Language Model (LLM). It's designed around a central list of prompts (`PROMPT_LIST` in `app/api/chat/prompts.ts`) that define the structure and behavior of the conversation. Each prompt object in this list (`PromptType`) contains configuration options that control various aspects of the interaction, including LLM calls, input validation, state management (memory and buffer), and conversation flow control (transitions, chaining).

The core logic resides in `app/api/chat/route.ts`, which orchestrates calls to helper utilities based on the current state and prompt configuration. State between user requests is managed via session cookies. The primary flow for standard interactions is handled by `handleNonStreamingFlow` in `app/api/chat/nonStreamingFlow.ts`.

## 2. `PromptType` Properties (from `app/api/chat/prompts.ts`)

The `PromptType` interface defines the configuration for each step in the conversation flow. Here's an explanation of each property:

-   **`prompt_text` (string):**
    -   **Purpose:** This is the core text sent to the LLM as the system prompt for the current turn.
    -   **Interaction:** Placeholders like `{memoryKey}` within this text are replaced with values from `namedMemory` by `injectNamedMemory` in `memoryUtils.ts` before being sent to the LLM.

-   **`validation` (boolean | string):**
    -   **Purpose:** Determines if and how the *user's input* (responding to the *previous* prompt) should be validated before proceeding.
    -   **Interaction:**
        -   If `true` or a string is provided, `nonStreamingFlow.ts` calls `validateInput` (in `openaiApiUtils.ts`).
        -   `validateInput` uses an LLM call with either the default or a custom validation instruction (if `validation` is a string template) to check the user's input.
        -   If validation fails, `generateRetryMessage` (`openaiApiUtils.ts`) might be called to ask the user to try again, and `RollbackOnValidationFailure` (`promptUtils.ts`) determines the next step based on `fallbackIndex`.

-   **`important_memory` (boolean):**
    -   **Purpose:** Marks the assistant's response generated by *this* prompt as important.
    -   **Interaction:**
        -   In `memoryUtils.ts`, `processAssistantResponseMemory` checks this flag. If true, it prepends "Important\_memory:" to the assistant's response *content* before it's used further or added to history (though the actual insertion into history context happens in `nonStreamingFlow` or `autoTransitionUtils`).
        -   In `bufferUtils.ts`, `manageBuffer` explicitly preserves messages starting with "Important\_memory:" when trimming the history, ensuring they aren't removed by the buffer limit.

-   **`autoTransitionHidden` (boolean):**
    -   **Purpose:** If `true`, after this prompt executes successfully, the system *automatically* proceeds to the *next* prompt without waiting for user input. The intermediate LLM call and response for this hidden step are not shown to the user.
    -   **Interaction:** Managed by `processTransitions` and `handleAutoTransitionHidden` in `autoTransitionUtils.ts`. These functions orchestrate the LLM call for the hidden prompt, update state (memory, buffer, index), and continue transitions if the next prompt also has an auto-transition flag. The final response sent to the user comes from the *last* prompt in the transition chain.

-   **`autoTransitionVisible` (boolean):**
    -   **Purpose:** Similar to `autoTransitionHidden`, but the response generated by *this* prompt *is* appended to the final output shown to the user before automatically proceeding to the next prompt.
    -   **Interaction:** Managed by `processTransitions` and `handleAutoTransitionVisible` in `autoTransitionUtils.ts`. Each visible transition's response is appended to the final output string. The chain continues until a prompt without an auto-transition flag is reached or an error occurs.

-   **`chaining` (boolean):**
    -   **Purpose:** Allows the output of one LLM call (from the prompt *before* this one) to be used as input context for the LLM call of *this* prompt, enabling sequential processing or refinement.
    -   **Interaction:** Handled by `chainIfNeeded` in `chainUtils.ts`. If a prompt has `chaining: true`, this function is called. It takes the previous assistant response, adds it to the history (often as a 'user' turn for context), updates the system prompt to the current one, calls the LLM, and repeats if the *next* prompt also has `chaining: true`. *(Note: Integration between `chainIfNeeded` and the main `processTransitions` flow might need review, as they handle sequential execution differently).*

-   **`temperature` (number):**
    -   **Purpose:** Controls the randomness/creativity of the LLM response for this specific prompt. Higher values (e.g., 0.8) mean more randomness, lower values (e.g., 0) mean more deterministic output.
    -   **Interaction:** The value is included in the payload sent to the LLM via `fetchApiResponseWithRetry` in `openaiApiUtils.ts`. If omitted, a default (often 0) is used.

-   **`buffer_memory` (number):**
    -   **Purpose:** Dynamically adjusts the size of the conversation history buffer *before* this prompt's LLM call is made.
    -   **Interaction:** `updateDynamicBufferMemory` in `memoryUtils.ts` reads this value. If present and valid, it updates the session's `currentBufferSize`. `manageBuffer` in `bufferUtils.ts` then uses this updated size to trim the history before the LLM call.

-   **`wait_time` (number):**
    -   **Purpose:** Intended to introduce a delay (potentially before or after the prompt executes).
    -   **Interaction:** Defined in the type, but **not currently implemented** in the provided utility functions (`autoTransitionUtils`, `nonStreamingFlow`, etc.).

-   **`addToDatabase` (boolean):**
    -   **Purpose:** Flag indicating whether the result of this interaction should be stored in a database.
    -   **Interaction:** Defined in the type. While `handleDatabaseStorageIfNeeded` exists (commented out in `route.ts` and potentially used in `nonStreamingFlow.ts`), its active usage and interaction with this flag **are not fully demonstrated** in the provided code. The `dbOptions` property likely relates to this.

-   **`model` (string):**
    -   **Purpose:** Specifies a particular LLM model (e.g., "gpt-4", "gpt-4o-mini") to use for *this specific prompt*, overriding the default.
    -   **Interaction:** `getModelForCurrentPrompt` (`promptUtils.ts`) retrieves this value. `fetchApiResponseWithRetry` (`openaiApiUtils.ts`) uses this model when making the API call. If omitted, `DEFAULT_OPENAI_MODEL` is used.

-   **`fallbackIndex` (number):**
    -   **Purpose:** Defines which prompt index the conversation should "roll back" to if validation fails for the user's input responding to *this* prompt. A value of 0 or undefined typically means re-ask the current question.
    -   **Interaction:** `RollbackOnValidationFailure` (`promptUtils.ts`) uses this value. In `nonStreamingFlow.ts`, if validation fails, this determines the `rolledBackIndex`. The session's `currentIndex` and `promptIndexThatAskedLastQuestion` are updated accordingly, and a retry message based on the rolled-back prompt is generated.

-   **`saveUserInputAs` (string):**
    -   **Purpose:** Specifies a key under which the user's raw input message should be saved into the `namedMemory`. This saving happens *after* successful validation of the input against the prompt that *asked* the question.
    -   **Interaction:** `saveUserInputToMemoryIfNeeded` (`memoryUtils.ts`) checks this property on the prompt that *asked* the question. If present, it updates the `namedMemory` object with the user's input using the specified string as the key.

-   **`saveAssistantOutputAs` (string):**
    -   **Purpose:** Specifies a key under which the *assistant's response* generated by *this* prompt should be saved into the `namedMemory`.
    -   **Interaction:** `processAssistantResponseMemory` (`memoryUtils.ts`) checks this property. If present, it saves the *cleaned* assistant response to the `namedMemory` object using the specified string as the key.

-   **`dbOptions` (object):**
    -   **Purpose:** Provides specific configuration for database storage (collection name, document ID, fields).
    -   **Interaction:** Intended for use with `handleDatabaseStorageIfNeeded`, but **not actively demonstrated** in the provided flow.

## 3. Core Flow (`nonStreamingFlow.ts`)

The `handleNonStreamingFlow` function orchestrates the main interaction logic when not streaming:

1.  **Load State:** Reads `currentIndex`, `promptIndexThatAskedLastQuestion`, `namedMemory`, and `currentBufferSize` from the session data.
2.  **Validation:** Retrieves the *last user message* and validates it against the requirements of the prompt at `promptIndexThatAskedLastQuestion` using `validateInput`.
3.  **Handle Validation Failure:** If invalid, uses `fallbackIndex` (via `RollbackOnValidationFailure`) to determine the next state, generates a `retryMessage`, updates the session, and returns immediately.
4.  **Save Valid User Input:** If valid, saves the user input to `namedMemory` if configured (`saveUserInputAs` on the *previous* prompt) using `saveUserInputToMemoryIfNeeded`.
5.  **Check Completion:** If `currentIndex` is beyond the end of `PROMPT_LIST`, return a completion message.
6.  **Prepare for LLM Call:**
    -   Gets the `currentPromptObj` at `currentIndex`.
    -   Updates `currentBufferSize` using `updateDynamicBufferMemory`.
    -   Injects `namedMemory` into the `currentPromptText` using `injectNamedMemory`.
    -   Prepares `historyForLLM` by adding the system prompt and managing the buffer using `manageBuffer`.
7.  **Main LLM Call:** Calls `fetchApiResponseWithRetry` with the prepared payload.
8.  **Process Response & Memory:**
    -   Cleans the response using `cleanLlmResponse`.
    -   Saves the assistant response to `namedMemory` if configured (`saveAssistantOutputAs`) and handles `important_memory` prefixing using `processAssistantResponseMemory`.
    -   Updates the history context.
9.  **Process Transitions:** Calls `processTransitions` (`autoTransitionUtils.ts`) to handle any `autoTransitionHidden` or `autoTransitionVisible` flags on the prompt that just executed (`currentIndex`). This might involve further LLM calls and state updates.
10. **Update Final State:** Updates local state variables (`finalResponseContent`, `nextIndexAfterProcessing`, `indexGeneratingFinalResponse`, `currentNamedMemory`, `currentBufferSize`) based on the transition results.
11. **Store & Return:** (Optionally stores results if `isStorable` is true) Returns the `finalResponseContent` and the `updatedSessionData` containing the state for the *next* turn (the `nextIndexAfterProcessing`, the `indexGeneratingFinalResponse` as the new `promptIndexThatAskedLastQuestion`, updated memory, and buffer size).

## 4. Helper Utility Roles

-   **`openaiApiUtils.ts`:** Handles all direct interactions with the OpenAI API: making calls (`fetchApiResponseWithRetry`), validating user input via LLM (`validateInput`), generating retry messages via LLM (`generateRetryMessage`), and cleaning LLM output (`cleanLlmResponse`). Also defines API constants.
-   **`memoryUtils.ts`:** Focuses on `namedMemory` management. Injects memory into prompts (`injectNamedMemory`), saves user input (`saveUserInputToMemoryIfNeeded`), saves/processes assistant output (`processAssistantResponseMemory`), and calculates dynamic buffer sizes based on prompt config (`updateDynamicBufferMemory`).
-   **`bufferUtils.ts`:** Manages the conversation history length (`manageBuffer`), ensuring it doesn't exceed the `currentBufferSize` while preserving the system prompt and `Important_memory` lines.
-   **`autoTransitionUtils.ts`:** Implements the logic for automatic conversation steps (`handleAutoTransitionHidden`, `handleAutoTransitionVisible`) and orchestrates sequences of these transitions (`processTransitions`).
-   **`promptUtils.ts`:** Contains helpers directly related to processing prompts, like selecting the correct LLM model (`getModelForCurrentPrompt`) and determining the rollback index on validation failure (`RollbackOnValidationFailure`).
-   **`chainUtils.ts`:** Implements the `chainIfNeeded` logic for sequentially processing prompts where one output feeds the next. *(As noted, its interplay with `processTransitions` might need clarification).*
-   **`config.ts`:** Appears to contain older/duplicated versions of the auto-transition handling functions found in `autoTransitionUtils.ts`. Might be redundant or outdated.
-   **`flowHelpers.ts`:** Currently empty, possibly intended for future helper functions related to the main flow.
-   **`namedMemoryUtils.ts`:** Contains functions (`insertImportantMemoryEntry`, `injectNamedMemory`) that seem to overlap significantly with functionality now present in `memoryUtils.ts` and potentially `nonStreamingFlow.ts` or `autoTransitionUtils.ts`. Might be redundant or from an earlier design iteration.

## 5. State Management

The system is stateful. `app/api/chat/route.ts` uses helper functions (presumably from `lib/session`, e.g., `getSessionCookieData`, `updateSessionCookieData`) to read the conversation state (`currentIndex`, `promptIndexThatAskedLastQuestion`, `namedMemory`, `currentBufferSize`) from session cookies at the start of a request and save the updated state back to cookies before sending the response. This allows the conversation to maintain context across multiple user turns.

## 6. Conclusion

The design provides a robust and highly configurable framework for building structured, multi-turn conversations. By defining behaviors within the `PROMPT_LIST`, developers can control validation, memory, flow control, and LLM parameters on a per-step basis. The separation of concerns into utility modules (API interaction, memory, buffer, transitions) promotes maintainability. Potential areas for review include the integration of the `chaining` mechanism with the primary `processTransitions` flow and consolidating potentially redundant code in `config.ts` and `namedMemoryUtils.ts`.