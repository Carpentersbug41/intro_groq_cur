import { NextRequest, NextResponse } from "next/server";

// ********************************************************************************
// *****      IMPORTANT: DO NOT REMOVE OR MODIFY THE CHAINING, VALIDATION, OR BUFFER CODE     *****
// ***** These sections are critical for the prompt-chaining flow and validation.  *****
// ********************************************************************************

export const runtime = "edge";

// ----------------------------
// You can tweak the number of messages to retain
// ----------------------------
const BUFFER_SIZE = 2; // e.g., 3 user + 3 assistant exchanges max

// ---------------------------------------------------------------------------------
// ***** DO NOT TOUCH: Variables for Conversation History and Prompt Index *****
// ---------------------------------------------------------------------------------
let conversationHistory: { role: string; content: string }[] = [];
let currentIndex = 0;

// ---------------------------------------------------------------------------------
// Prompt List for Demonstration
// (Optionally set validation: true if you want to run prompt validation.)
// ---------------------------------------------------------------------------------
const PROMPT_LIST = [
  {
    prompt_text: "#System message:\n Ask the user their favourite book.",
    chaining: false,
    important_memory: true // This response will be saved as important memory
  },

  {
    prompt_text: "#System message:\n Ask the user if they like beer.",
    chaining: false,
    important_memory: true // This response will be saved as important memory
  },

  {
    prompt_text: "#System message:\n Ask the user if they like incels.",
    chaining: false,
    // validation: true,
  },

  {
    prompt_text: "#System message:\n Ask the user to input a number",
    // chaining: true,
    validation: true,
  },
  {
    prompt_text: "#System message:\n  add 2 to the number you have.",
    chaining: true,
    // validation: true,
  },
  {
    prompt_text: "#System message:\n Take the result of the last operation and multiply it by 3",
    chaining: true,
    // validation: true,
  },
  {
    prompt_text: "#System message:\n Convert the result into cats. For example 'There are 5 cats.'",
    chaining: true,
    // validation: true,
  },
  {
    prompt_text: "#System message:\n Add a colour to the cats. Give them all the same colour. For example 'There are 5 red cats.'",
    chaining: true,
    // validation: true,
  },
  {
    prompt_text: "#System message:\n Ask the user their favourite animal.",
    chaining: false,
    // validation: true,
  },
];

// ---------------------------------------------------------------------------------
// 1) MINIMAL VALIDATION CALL (DO NOT MODIFY THIS FUNCTION)
// ---------------------------------------------------------------------------------
async function validateInput(userInput: string, currentPrompt: string) {
  // ***** CRITICAL VALIDATION SECTION: DO NOT EDIT OR REMOVE *****
  const validationInstruction = `
    You are a validation assistant.
    Your task is to assess if the user's input is loosely related to the prompt requirements.  Don't be too strict with this.  
    As long as the user answered it loosely it is valid.  If the answer is completely unrelated it isn't valid.
    Current prompt: '${currentPrompt}'
    User input: '${userInput}'
    
    Respond with only one word: "VALID" if the input matches the prompt's requirement,
    or "INVALID" if it does not.
    Do not provide any additional explanation or description.
  `;

  console.log("\n[DEBUG] Validation Payload (Minimal):", { userInput, currentPrompt });

  const payload = {
    model: "llama-3.3-70b-versatile",
    messages: [{ role: "system", content: validationInstruction }],
  };

  const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${process.env.GROQ_API_KEY}`,
    },
    body: JSON.stringify(payload),
  });

  if (!response.ok) {
    const errorText = await response.text();
    console.error("[ERROR] Validation API call failed:\n", errorText);
    throw new Error(`Validation API error: ${response.statusText}`);
  }

  const responseData = await response.json();
  const validationResult = responseData.choices?.[0]?.message?.content?.trim() || "INVALID";
  console.log("[DEBUG] Validation Result:", validationResult);

  return validationResult === "VALID";
}

// ---------------------------------------------------------------------------------
// 2) RETRY MESSAGE GENERATOR IF INVALID (DO NOT MODIFY THIS SECTION)
// ---------------------------------------------------------------------------------
async function generateRetryMessage(userInput: string, currentPrompt: string) {
  // ***** CRITICAL VALIDATION SECTION: DO NOT EDIT OR REMOVE *****
  console.log("\n[DEBUG] Generating Retry Message for invalid input:", userInput);

  const payload = {
    model: "llama-3.3-70b-versatile",
    messages: [
      { role: "system", content: currentPrompt },
      // We re-use conversation history minus the system message
      ...conversationHistory.filter((entry) => entry.role !== "system"),
    ],
  };

  const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${process.env.GROQ_API_KEY}`,
    },
    body: JSON.stringify(payload),
  });

  if (!response.ok) {
    const errorText = await response.text();
    console.error("[ERROR] Retry API call failed:\n", errorText);
    throw new Error(`Retry API error: ${response.statusText}`);
  }

  const responseData = await response.json();
  const retryContent = responseData.choices?.[0]?.message?.content;

  if (!retryContent) {
    throw new Error("Invalid retry response: No content.");
  }

  console.log("[DEBUG] Retry Message Generated:\n", retryContent);
  return retryContent;
}

// ---------------------------------------------------------------------------------
// 3) BASIC FETCH TO THE API (UNTOUCHED)
// ---------------------------------------------------------------------------------
async function fetchApiResponse(payload: any): Promise<string | null> {
  // This function is used for normal / direct API fetches
  console.log("\n[DEBUG] Basic fetchApiResponse call with payload:\n", JSON.stringify(payload, null, 2));
  try {
    const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${process.env.GROQ_API_KEY}`,
      },
      body: JSON.stringify(payload),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error("[ERROR] fetchApiResponse call failed:\n", errorText);
      return null;
    }

    const responseData = await response.json();
    return responseData.choices?.[0]?.message?.content || null;
  } catch (error: any) {
    console.error("[ERROR] in fetchApiResponse:\n", error.message);
    return null;
  }
}

// ---------------------------------------------------------------------------------
// 4) chainIfNeeded LOGIC (DO NOT MODIFY ANYTHING INSIDE THIS FUNCTION)
// ---------------------------------------------------------------------------------
async function chainIfNeeded(assistantContent: string): Promise<string | null> {
  // ***** CRITICAL CHAINING SECTION: DO NOT EDIT OR REMOVE *****
  let chainResponse = assistantContent;

  while (true) {
    if (currentIndex >= PROMPT_LIST.length) {
      console.log("[CHAIN DEBUG] No more prompts to chain.");
      return chainResponse;
    }
    if (!PROMPT_LIST[currentIndex]?.chaining) {
      console.log("[CHAIN DEBUG] Next prompt is NOT chaining. Stopping chain here.");
      return chainResponse;
    }

    console.log("[CHAIN DEBUG] The next prompt is chaining=true, so we feed the last assistant output as user input.");

    // Grab next system prompt
    const systemPrompt = PROMPT_LIST[currentIndex]?.prompt_text || "No further prompts.";

    // 1) Mark assistant's last output as a 'user' message for chaining
    conversationHistory.push({ role: "user", content: chainResponse });

    // 2) Remove duplicates if there's an "assistant => user" with same content
    conversationHistory = conversationHistory.filter((entry, index, self) => {
      return !(
        entry.role === "assistant" &&
        self[index + 1]?.role === "user" &&
        self[index + 1]?.content === entry.content
      );
    });

    console.log("[CHAIN DEBUG] conversationHistory AFTER removing duplicates:\n", JSON.stringify(conversationHistory, null, 2));

    // 3) Build the chain payload, re-labelling user lines that originated as assistant
    const tempHistory = [
      { role: "system", content: systemPrompt },
      ...conversationHistory.map((entry) => {
        const wasAssistantBefore = conversationHistory.some(
          (msg) => msg.role === "assistant" && msg.content === entry.content
        );
        if (entry.role === "user" && wasAssistantBefore) {
          // Convert user->assistant if it was previously from an assistant
          return { role: "assistant", content: entry.content };
        }
        return entry; // else keep as is
      }),
    ];

    // Log the chain payload
    console.log("[CHAIN DEBUG] Next chaining payload to LLM:\n", JSON.stringify(tempHistory, null, 2));

    const chainPayload = {
      model: "llama-3.3-70b-versatile",
      messages: tempHistory,
    };

    // Move index forward so future chaining references the next prompt
    currentIndex++;

    // 4) Make the chain request
    const chainResp = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${process.env.GROQ_API_KEY}`,
      },
      body: JSON.stringify(chainPayload),
    });

    if (!chainResp.ok) {
      const errorText = await chainResp.text();
      console.error("[CHAIN DEBUG] Chaining request failed:\n", errorText);
      return chainResponse; // fallback to the last known content
    }

    const chainData = await chainResp.json();
    const newContent = chainData.choices?.[0]?.message?.content;

    if (!newContent) {
      console.warn("[CHAIN DEBUG] No content returned from chain. Stopping chaining.");
      return chainResponse;
    }

    console.log("[CHAIN DEBUG] newAssistantContent from chain:", newContent);

    // 5) Append the new chain response as 'assistant'
    conversationHistory.push({ role: "assistant", content: newContent });

    // 6) Update chainResponse for next iteration
    chainResponse = newContent;
  }
}

// ---------------------------------------------------------------------------------
// 5) MAIN POST HANDLER (WITH OPTIONAL VALIDATION CHECK + BUFFER MANAGEMENT)
// ---------------------------------------------------------------------------------
export async function POST(req: NextRequest) {
  try {
    const body = await req.json();
    const userMessage = body.message?.trim();

    console.log("[INFO] Received User Input:", userMessage || "[No Input Provided]");
    if (!userMessage) {
      console.log("[WARN] No User Input Received. Returning Error 400.");
      return new Response("No input received. Please try again.", { status: 400 });
    }

    // 1) If no more prompts, we end the conversation
    const currentPrompt = PROMPT_LIST[currentIndex]?.prompt_text;
    // Optionally check if this prompt requires validation
    const promptValidationNeeded = PROMPT_LIST[currentIndex]?.validation || false;

    if (!currentPrompt) {
      const finalMsg = "Thank you for your responses! Goodbye.";
      conversationHistory.push({ role: "assistant", content: finalMsg });
      console.log("[INFO] Conversation ended (no more prompts).");
      return new Response(finalMsg, { status: 200 });
    }

    console.log("[DEBUG] Current Prompt is:\n", currentPrompt);
    console.log("[DEBUG] Storing user message in conversationHistory:\n", userMessage);

    // 2) Push the user message
    conversationHistory.push({ role: "user", content: userMessage });

    // 3) If this prompt requires validation, run validation now:
    if (promptValidationNeeded) {
      console.log("[DEBUG] This prompt has 'validation: true', running validation...");
      const isValid = await validateInput(userMessage, currentPrompt);
      if (!isValid) {
        console.log("[DEBUG] Validation failed, generating retry message.");
        const retryMsg = await generateRetryMessage(userMessage, currentPrompt);
        conversationHistory.push({ role: "assistant", content: retryMsg });
        return new Response(retryMsg, { status: 200 });
      }
    } else {
      console.log("[DEBUG] This prompt does NOT require validation. Skipping validation check.");
    }

    // 4) Advance the current index to the next prompt
    currentIndex++;

    // 5) APPLY BUFFER MANAGEMENT after user push, before the main LLM call
    conversationHistory = manageBuffer(conversationHistory);

    // 6) Build the main payload for the LLM
    const payload = {
      model: "llama-3.3-70b-versatile",
      messages: [{ role: "system", content: currentPrompt }, ...conversationHistory],
    };

    console.log("[DEBUG] Main Payload to LLM:\n", JSON.stringify(payload, null, 2));

    // 7) Make the main API call
    const mainResp = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${process.env.GROQ_API_KEY}`,
      },
      body: JSON.stringify(payload),
    });

    if (!mainResp.ok) {
      const errorText = await mainResp.text();
      console.error("[ERROR] Main LLM API call failed:\n", errorText);
      return new Response("I'm sorry, I couldn't process that. Please try again.", { status: 200 });
    }

    const mainData = await mainResp.json();
    const assistantContent = mainData.choices?.[0]?.message?.content;

    if (!assistantContent) {
      console.warn("[WARN] LLM returned no content. Stopping.");
      return new Response("No content returned. Please try again.", { status: 200 });
    }

    console.log("[DEBUG] assistantContent from main call:", assistantContent);

    // 8) Add new assistant content to the conversationHistory
    conversationHistory.push({ role: "assistant", content: assistantContent });
    console.log("[DEBUG] Updated conversationHistory after main call:\n", JSON.stringify(conversationHistory, null, 2));

    // 9) APPLY BUFFER MANAGEMENT again after we have the new assistant message
    conversationHistory = manageBuffer(conversationHistory);

    // 10) Possibly chain to next prompts
    const finalChained = await chainIfNeeded(assistantContent);

    // 11) One last time, re-check buffer after chaining
    conversationHistory = manageBuffer(conversationHistory);

    // [ADDED CODE START] -----------------------------------------------
    // Insert an Important_memory line if the *previous* prompt was marked important_memory: true
    if (PROMPT_LIST[currentIndex - 1]?.important_memory) {
      // finalChained is the final answer after chaining (if any). If no chaining, fall back to assistantContent.
      const memoryContent = finalChained || assistantContent;

      // Identify the last system message's content so we can locate it in the conversationHistory
      const lastPrompt = PROMPT_LIST[currentIndex - 1]?.prompt_text;
      const systemIndex = conversationHistory.findIndex(
        (entry) => entry.role === "system" && entry.content === lastPrompt
      );

      // Only insert if we found the correct system message
      if (systemIndex !== -1) {
        conversationHistory.splice(systemIndex + 1, 0, {
          role: "assistant",
          content: `Important_memory: ${memoryContent}`,
        });
      }

      // Re-check buffer after splicing in the Important_memory line, so it's preserved
      conversationHistory = manageBuffer(conversationHistory);
    }
    // [ADDED CODE END] -------------------------------------------------

    // 12) Return the final result (chained or immediate)
    return new Response(finalChained || assistantContent, { status: 200 });
  } catch (err: any) {
    console.error("[ERROR] in POST handler:\n", err.message);
    return NextResponse.json({ error: err.message }, { status: 500 });
  }
}

/**
 * ---------------------------------------------------------------------------------
 * DO NOT TOUCH: BUFFER MANAGEMENT SECTION
 * ---------------------------------------------------------------------------------
 * This function ensures only the most recent messages remain in history,
 * while preserving lines that start with "Important_memory:"
 * and the current system message.
 */
function manageBuffer(conversationHistory: { role: string; content: string }[]) {
  console.log(`[DEBUG] Current Conversation History Length: ${conversationHistory.length}`);

  // 1) Identify the system message (must be preserved)
  const systemMessage = conversationHistory.find((entry) => entry.role === "system");

  // 2) Lines that start with "Important_memory:" must also be preserved
  const importantMemoryLines = conversationHistory.filter(
    (entry) =>
      entry.role === "assistant" &&
      entry.content.trim().startsWith("Important_memory:")
  );

  // 3) All other lines
  const otherMessages = conversationHistory.filter(
    (entry) => entry !== systemMessage && !importantMemoryLines.includes(entry)
  );

  // 4) If the total "other" lines exceed BUFFER_SIZE, trim from the oldest
  if (otherMessages.length > BUFFER_SIZE) {
    const excessCount = otherMessages.length - BUFFER_SIZE;
    console.log(`[DEBUG] Trimming ${excessCount} oldest non-system, non-important messages.`);

    // Retain only the newest BUFFER_SIZE from 'other'
    const trimmed = otherMessages.slice(excessCount);

    // Reconstruct final array
    const finalHistory = [systemMessage, ...importantMemoryLines, ...trimmed].filter(Boolean);

    console.log(
      "[DEBUG] Trimmed Conversation History (with Important Memory Kept):",
      JSON.stringify(finalHistory, null, 2)
    );
    return finalHistory;
  }

  // 5) If under limit, do nothing
  return conversationHistory;
}
