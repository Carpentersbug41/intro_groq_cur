import { FlowStep, SessionState, ActionResult } from '@/app/api/chat/flows/types';
import { callLlm } from '../../utils/llmUtils'; // We will create/verify this next
import * as fs from 'fs/promises';
import * as path from 'path';
import mustache from 'mustache';

export const llmTransformHandler = {
  execute: async (step: FlowStep, state: SessionState, requestId: string): Promise<ActionResult> => {
    const log = (message: string) => console.log(`[req:${requestId}][LLM_TRANSFORM] ${message}`);
    if (!step.prompt_template) {
      throw new Error(`[llmTransformHandler] Step ${step.id} is missing a prompt_template.`);
    }

    // --- Template Rendering ---
    const templatePath = path.join(process.cwd(), 'app/api/chat', step.prompt_template);
    const template = await fs.readFile(templatePath, 'utf-8');
    const systemPrompt = mustache.render(template, { memory: state.namedMemory });
    
    // --- History Buffering (Simplified for now) ---
    const context = [...state.conversationHistory];

    // --- CRITICAL: Add Logging ---
    log(`--- Preparing to call LLM for step: ${step.id} ---`);
    log(`System Prompt: "${systemPrompt.substring(0, 200)}..."`);
    log(`Conversation Context: ` + JSON.stringify(context, null, 2));
    log(`--- End of Payload ---`);

    // --- Call the LLM ---
    const llmResponse = await callLlm(systemPrompt, context, step.model, step.temperature);
    log(`LLM Response Received: "${llmResponse.substring(0, 100)}..."`);

    // --- State Update ---
    const stateUpdates: Partial<SessionState> = {
      namedMemory: { ...state.namedMemory },
    };

    if (step.save_to_memory_key) {
      stateUpdates.namedMemory![step.save_to_memory_key] = llmResponse;
    }
    
    // The runner handles the next step, this handler should not set it.
    // stateUpdates.currentStepId = step.next_step; // <-- This is WRONG. The runner does this.

    return {
      contentForUser: null, // This action is silent to the user.
      stateUpdates,
    };
  }
}; 